{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a000c3f9",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here: implement variance ranking and \n",
    "# implement variance ranking in supervised baselines as well -> new notebook for just this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b1e8c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import image\n",
    "import seaborn as sns\n",
    "import glob\n",
    "from scipy.stats.stats import pearsonr\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.metrics import mean_absolute_error, accuracy_score, f1_score, balanced_accuracy_score, mean_squared_error, r2_score\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74e0d4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# which datasets to read and write\n",
    "\n",
    "do_indian_pines = False \n",
    "do_salient_objects = True\n",
    "do_plastic_flakes = True\n",
    "do_soil_moisture = False\n",
    "do_foods = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "plastic_flakes_sample = 0.1\n",
    "indian_pines_sample = 1 \n",
    "salient_objects_sample = 0.01\n",
    "foods_sample = 1\n",
    "soil_moisture_sample = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_b_kept = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9110b99a",
   "metadata": {},
   "source": [
    "## Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cacd65d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reward functions\n",
    "\n",
    "def calculate_correlations(data, num_bands_originally, num_bands_kept):\n",
    "    \n",
    "    #selected_bands = np.random.randint(0,num_bands_originally,num_bands_kept)\n",
    "    selected_bands = np.arange(0, data.shape[-1])\n",
    "    #print(selected_bands)    \n",
    "    corr_sum = 0\n",
    "    for i in selected_bands:\n",
    "        for j in selected_bands:\n",
    "            if i != j:\n",
    "                corr_sum += np.abs(pearsonr(data[:, i], \n",
    "                                   data[:, j])[0])\n",
    "            \n",
    "    return corr_sum/(len(selected_bands)**2)\n",
    "\n",
    "\n",
    "def calculate_mutual_infos(data, num_bands_originally, num_bands_kept):\n",
    "    \n",
    "    #selected_bands = np.random.randint(0,num_bands_originally,num_bands_kept)\n",
    "    selected_bands = np.arange(0, data.shape[-1])\n",
    "    #print(selected_bands)\n",
    "    normalized_mutual_info_score_sum = 0\n",
    "    for i in selected_bands:\n",
    "        for j in selected_bands:\n",
    "            if i != j:\n",
    "                normalized_mutual_info_score_sum += normalized_mutual_info_score(data[:, i],\n",
    "                                                                             data[:, j])\n",
    "            \n",
    "    return normalized_mutual_info_score_sum/(len(selected_bands)**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2c8d6b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## dataset loading\n",
    "\n",
    "def load_datasets(Dataset):\n",
    "    \n",
    "    if Dataset == 'SM':\n",
    "        hyper_path = '../data/soil_moisture/hyperspectral_imagery/*npy'\n",
    "        hyper = np.load(glob.glob(hyper_path)[0])\n",
    "        gt_path = '../data/soil_moisture/gt_labels/*npy'\n",
    "        gt = np.load(glob.glob(gt_path)[0])\n",
    "        return hyper, gt\n",
    "    \n",
    "    if Dataset == 'IN':\n",
    "        hyper_path = '../data/indian_pines/hyperspectral_imagery/*npy'\n",
    "        hyper = np.load(glob.glob(hyper_path)[0])\n",
    "        gt_path = '../data/indian_pines/gt_labels/*npy'\n",
    "        gt = np.load(glob.glob(gt_path)[0])\n",
    "        return hyper, gt\n",
    "    \n",
    "    if Dataset == 'SO':\n",
    "        hyper_path = '../data/salient_objects/hyperspectral_imagery/*npy'\n",
    "        gt_path = '../data/salient_objects/gt_labels/*npy'\n",
    "        hypers=[]\n",
    "        gt_labels=[]\n",
    "        for i in range(len(glob.glob(hyper_path))):\n",
    "            hyper = np.load(glob.glob(hyper_path)[i])\n",
    "            hypers.append(hyper)\n",
    "            gt = np.load(glob.glob(gt_path)[i])\n",
    "            gt_labels.append(gt)\n",
    "        return hypers, gt_labels\n",
    "\n",
    "                          \n",
    "    if Dataset == 'PF':\n",
    "        hyper_path = '../data/plastic_flakes/hyperspectral_imagery/*npy'\n",
    "        gt_path = '../data/plastic_flakes/gt_labels/*npy'\n",
    "        hypers=[]\n",
    "        gt_labels=[]\n",
    "        for i in range(len(glob.glob(hyper_path))):\n",
    "            hyper = np.load(glob.glob(hyper_path)[i])\n",
    "            hypers.append(hyper)\n",
    "            gt = np.load(glob.glob(gt_path)[i])\n",
    "            gt_labels.append(gt)\n",
    "        return hypers, gt_labels\n",
    "    \n",
    "    if Dataset == 'Foods':\n",
    "        hyper_path = '../data/foods/hyperspectral_imagery/*npy'\n",
    "        gt_path = '../data/foods/gt_labels/*npy'\n",
    "        hypers=[]\n",
    "        gt_labels=[]\n",
    "        for i in range(len(glob.glob(hyper_path))):\n",
    "            hyper = np.load(glob.glob(hyper_path)[i])\n",
    "            hypers.append(hyper)\n",
    "            gt = np.load(glob.glob(gt_path)[i])\n",
    "            gt_labels.append(gt)\n",
    "        return hypers, gt_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978c3cd0",
   "metadata": {},
   "source": [
    "## Plastic flakes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "06f64bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset info...\n",
      "The shape of the original imagery: (11, 112128, 224)\n",
      "The shape of the original labels: (11, 112128)\n",
      "The shape of the vertically stacked images: (1233408, 224)\n",
      "The shape of the vertically stacked images: (1233408,)\n"
     ]
    }
   ],
   "source": [
    "# stacks all images vertically\n",
    "\n",
    "# load data\n",
    "\n",
    "if do_plastic_flakes:\n",
    "    \n",
    "    hyper, gt = load_datasets(\n",
    "        'PF')\n",
    "    \n",
    "    hyper, gt = np.array(hyper), np.array(gt)\n",
    "    \n",
    "    hyper_multiple = np.empty([hyper.shape[0]*hyper.shape[1], hyper.shape[-1]])\n",
    "    gt_multiple = np.empty([gt.shape[0]*gt.shape[1]])\n",
    "    \n",
    "    print('\\nDataset info...')\n",
    "    print('The shape of the original imagery:', hyper.shape)\n",
    "    print('The shape of the original labels:', gt.shape)\n",
    "    \n",
    "    for i in range(hyper.shape[0]):\n",
    "        hyper_multiple[i*hyper.shape[1]:(i+1)*hyper.shape[1] , :] = hyper[i, :, :]\n",
    "        gt_multiple[i*hyper.shape[1]:(i+1)*hyper.shape[1]] = gt[i, :]\n",
    "\n",
    "    print('The shape of the vertically stacked images:', hyper_multiple.shape)\n",
    "    print('The shape of the vertically stacked images:', gt_multiple.shape)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7bf8dd6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the sub-sampled vertically stacked images: (123340, 224)\n",
      "[188 192 193 197 198 199 200 201 202 196 204 205 206 207 208 209 210 211\n",
      " 212 213 214 215 216 217 218 219 195 194 187 203]\n",
      "[0.9280932501728089]\n",
      "\n",
      "Correlation reward 0.9280932501728089\n",
      "[188 192 193 197 198 199 200 201 202 196 204 205 206 207 208 209 210 211\n",
      " 212 213 214 215 216 217 218 219 195 194 187 203]\n",
      "[0.5756094262873841]\n",
      "Normalized mutual information reward 0.5756094262873841\n"
     ]
    }
   ],
   "source": [
    "# rewards\n",
    "    \n",
    "if do_plastic_flakes:\n",
    "        \n",
    "    # randomly sample hyper_multiple for 5% of the pixels\n",
    "    indices = np.random.randint(0, hyper_multiple.shape[0], int(hyper_multiple.shape[0]*plastic_flakes_sample))\n",
    "    hyper_multiple = hyper_multiple[indices, :]\n",
    "    print('The shape of the sub-sampled vertically stacked images:', hyper_multiple.shape)\n",
    "    \n",
    "    correlations = []\n",
    "    #for i in range(num_runs):\n",
    "    correlations.append(calculate_correlations(hyper_multiple, num_bands_originally=hyper_multiple.shape[-1], num_bands_kept=num_b_kept))\n",
    "    print(correlations)\n",
    "    print(f'\\nCorrelation reward', np.mean(correlations))\n",
    "    \n",
    "    mis = []\n",
    "    #for i in range(num_runs):\n",
    "    mis.append(calculate_mutual_infos(hyper_multiple, num_bands_originally=hyper_multiple.shape[-1], num_bands_kept=num_b_kept))\n",
    "    print(mis)\n",
    "    print(f'Normalized mutual information reward', np.mean(mis))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b213af",
   "metadata": {},
   "source": [
    "## Salient objects dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a5364f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset info...\n",
      "The shape of the original imagery: (60, 786432, 81)\n",
      "The shape of the original labels: (60, 786432)\n",
      "\n",
      "Dataset info...\n",
      "The shape of the vertically stacked images: (47185920, 81)\n",
      "The shape of the vertically stacked images: (47185920,)\n"
     ]
    }
   ],
   "source": [
    "# stacks all images vertically\n",
    "\n",
    "# load data\n",
    "\n",
    "if do_salient_objects:\n",
    "    \n",
    "    hyper, gt = load_datasets(\n",
    "        'SO')\n",
    "    \n",
    "    hyper, gt = np.array(hyper), np.array(gt)\n",
    "    \n",
    "    hyper_multiple = np.empty([hyper.shape[0]*hyper.shape[1], hyper.shape[-1]])\n",
    "    gt_multiple = np.empty([gt.shape[0]*gt.shape[1]])\n",
    "    \n",
    "    print('\\nDataset info...')\n",
    "    print('The shape of the original imagery:', hyper.shape)\n",
    "    print('The shape of the original labels:', gt.shape)\n",
    "    \n",
    "    for i in range(hyper.shape[0]):\n",
    "        hyper_multiple[i*hyper.shape[1]:(i+1)*hyper.shape[1] , :] = hyper[i, :, :]\n",
    "        gt_multiple[i*hyper.shape[1]:(i+1)*hyper.shape[1]] = gt[i, :]\n",
    "\n",
    "    print('\\nDataset info...')\n",
    "    print('The shape of the vertically stacked images:', hyper_multiple.shape)\n",
    "    print('The shape of the vertically stacked images:', gt_multiple.shape)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "20d99755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the sub-sampled vertically stacked images: (471859, 81)\n",
      "[50 28 40 49 48 47 46 45 44 43 42 41 39 38 37 36 35 34 33 32 71 31 30 29\n",
      " 27 76 23 22 21 80]\n",
      "\n",
      "Correlation reward 0.12688449348487058\n",
      "[50 28 40 49 48 47 46 45 44 43 42 41 39 38 37 36 35 34 33 32 71 31 30 29\n",
      " 27 76 23 22 21 80]\n",
      "Normalized mutual information reward 0.1394126638935471\n"
     ]
    }
   ],
   "source": [
    "# rewards\n",
    "    \n",
    "if do_salient_objects:\n",
    "        \n",
    "    # randomly sample hyper_multiple for 1% of the pixels\n",
    "    indices = np.random.randint(0, hyper_multiple.shape[0], int(hyper_multiple.shape[0]*salient_objects_sample))\n",
    "    hyper_multiple = hyper_multiple[indices, :]\n",
    "    print('The shape of the sub-sampled vertically stacked images:', hyper_multiple.shape)\n",
    "    \n",
    "    correlations = []\n",
    "    #for i in range(num_runs):\n",
    "    correlations.append(calculate_correlations(hyper_multiple, num_bands_originally=hyper_multiple.shape[-1], num_bands_kept=num_b_kept))\n",
    "    print(f'\\nCorrelation reward', np.mean(correlations))\n",
    "    \n",
    "    mis = []\n",
    "    #for i in range(num_runs):\n",
    "    mis.append(calculate_mutual_infos(hyper_multiple, num_bands_originally=hyper_multiple.shape[-1], num_bands_kept=num_b_kept))\n",
    "    print(f'Normalized mutual information reward', np.mean(mis))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indian Pines dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "if do_indian_pines:\n",
    "    \n",
    "    hyper, gt = load_datasets(\n",
    "        'IN')\n",
    "    \n",
    "    print('\\nDataset info...')\n",
    "    print('The shape of the original imagery:', hyper.shape)\n",
    "    print('The shape of the original labels:', gt.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rewards\n",
    "    \n",
    "if do_indian_pines:\n",
    "    num_runs = 50\n",
    "    \n",
    "    correlations = []\n",
    "    #for i in range(num_runs):\n",
    "    correlations.append(calculate_correlations(hyper, num_bands_originally=hyper.shape[-1], num_bands_kept=num_b_kept))\n",
    "    print(f'\\nCorrelation reward', np.mean(correlations))\n",
    "    \n",
    "    mis = []\n",
    "    #for i in range(num_runs):\n",
    "    mis.append(calculate_mutual_infos(hyper, num_bands_originally=hyper.shape[-1], num_bands_kept=num_b_kept))\n",
    "    print(f'Normalized mutual information reward', np.mean(mis))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soil moisture dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0ca19147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "if do_soil_moisture:\n",
    "    \n",
    "    hyper, gt = load_datasets(\n",
    "        'SM')\n",
    "    \n",
    "    print('\\nDataset info...')\n",
    "    print('The shape of the original imagery:', hyper.shape)\n",
    "    print('The shape of the original labels:', gt.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rewards\n",
    "    \n",
    "if do_soil_moisture:\n",
    "    num_runs = 50\n",
    "    \n",
    "    correlations = []\n",
    "    #for i in range(num_runs):\n",
    "    correlations.append(calculate_correlations(hyper, num_bands_originally=hyper.shape[-1], num_bands_kept=num_b_kept))\n",
    "    print(f'\\nCorrelation reward', np.mean(correlations))\n",
    "    \n",
    "    mis = []\n",
    "    #for i in range(num_runs):\n",
    "    mis.append(calculate_mutual_infos(hyper, num_bands_originally=hyper.shape[-1], num_bands_kept=num_b_kept))\n",
    "    print(f'Normalized mutual information reward', np.mean(mis))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Foods dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "if do_foods:\n",
    "    \n",
    "    hyper, gt = load_datasets(\n",
    "        'Foods')\n",
    "\n",
    "    hyper, gt = hyper[0], gt[0]\n",
    "    \n",
    "    print('\\nDataset info...')\n",
    "    print('The shape of the original imagery:', hyper.shape)\n",
    "    print('The shape of the original labels:', gt.shape)\n",
    "    \n",
    "# rewards\n",
    "    \n",
    "if do_indian_pines:\n",
    "    num_runs = 50\n",
    "    \n",
    "    correlations = []\n",
    "    #for i in range(num_runs):\n",
    "    correlations.append(calculate_correlations(hyper, num_bands_originally=hyper.shape[-1], num_bands_kept=num_b_kept))\n",
    "    print(f'\\nCorrelation reward', np.mean(correlations))\n",
    "    \n",
    "    mis = []\n",
    "    #for i in range(num_runs):\n",
    "    mis.append(calculate_mutual_infos(hyper, num_bands_originally=hyper.shape[-1], num_bands_kept=num_b_kept))\n",
    "    print(f'Normalized mutual information reward', np.mean(mis))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "301faebbd5cea7fd4466786a19f1bea9d8baf657aaca95ef39840c46b8697603"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
