{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78720978",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import h5py\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import accuracy_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import torch.nn as nn\n",
    "from scipy.stats.stats import pearsonr\n",
    "import torch\n",
    "from matplotlib.pyplot import figure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4cf4d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataManager():\n",
    "    \n",
    "    def __init__(self, params, num_bands):\n",
    "        \n",
    "        self.rl_data = None\n",
    "        self.dataset_type = params['dataset_type']\n",
    "        self.data_file_path = params['data_file_path']\n",
    "        self.num_bands = num_bands\n",
    "        \n",
    "        #load the data\n",
    "        assert self.dataset_type in ('IndianPines', 'Botswana', 'SalientObjects'), f'{self.dataset_type} is not valid' \n",
    "        #separating out in case any of the data requires unique pre-processig\n",
    "        if self.dataset_type == 'IndianPines':\n",
    "            self.load_indian_pine_data()\n",
    "        elif self.dataset_type == 'Botswana':\n",
    "            self.load_botswana_data()\n",
    "        elif self.dataset_type == 'SalientObjects':\n",
    "            self.load_salient_objects_data()\n",
    "            \n",
    "        #self.x_train = None\n",
    "        #self.y_train = None\n",
    "        #self.x_test = None\n",
    "        #self.y_test = None\n",
    "\n",
    "    def load_indian_pine_data(self):        \n",
    "\n",
    "        hyper_path = '../data/indian_pines/hyperspectral_imagery/indian_pines_corrected.npy'\n",
    "        hyper = np.load(hyper_path)\n",
    "        print(hyper.shape)\n",
    "\n",
    "        # randomly sample for x% of the pixels\n",
    "        indices = np.random.randint(0, hyper.shape[0], int(hyper.shape[0]*0.01))\n",
    "        self.rl_data = hyper[indices, :]\n",
    "        print(self.rl_data.shape)\n",
    "\n",
    "\n",
    "    def load_salient_objects_data(self):        \n",
    "\n",
    "        hyper_path = '../data/salient_objects/hyperspectral_imagery/0001.npy'\n",
    "        hyper = np.load(hyper_path)\n",
    "        print(hyper.shape)\n",
    "\n",
    "        # randomly sample for x% of the pixels\n",
    "        indices = np.random.randint(0, hyper.shape[0], int(hyper.shape[0]*0.01))\n",
    "        self.rl_data = hyper[indices, :]\n",
    "        print(self.rl_data.shape)\n",
    "\n",
    "        \n",
    "    def load_botswana_data(self):\n",
    "        \n",
    "        self.rl_data = scipy.io.loadmat(self.data_file_path)\n",
    "\n",
    "    #def load_salient_objects(self)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "433ba101",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    \n",
    "    def __init__(self, size=10000):\n",
    "        self.size = size\n",
    "        self.paths = []\n",
    "        \n",
    "    def add_trajectories(self, paths):\n",
    "        self.paths.extend(paths)\n",
    "        self.paths = self.paths[-self.size:]\n",
    "        \n",
    "    def sample_buffer_random(self, num_trajectories):\n",
    "        \n",
    "        rand_idx = np.random.permutation(len(self.paths))[:num_trajectories]\n",
    "        return [self.paths[i] for i in rand_idx]\n",
    "        #return self.paths[rand_idx]\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c496803",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \n",
    "    def __init__(self, params):\n",
    "        \n",
    "        self.agent_params = params['agent']\n",
    "        self.num_bands = self.agent_params['num_bands']\n",
    "        self.n_iter = self.agent_params['n_iter']\n",
    "        self.trajectory_sample_size = self.agent_params['trajectory_sample_size']\n",
    "        self.batch_size = self.agent_params['batch_size']\n",
    "        self.num_critic_updates = self.agent_params['num_critic_updates']\n",
    "        \n",
    "        valid_rewards = ['correlation', 'mutual_info']\n",
    "        assert self.agent_params['reward_type'] in valid_rewards, 'rewards must be one of ' + valid_rewards.join(',') \n",
    "        \n",
    "        if self.agent_params['reward_type'] == 'correlation':\n",
    "            self.reward_func = self.calculate_correlations\n",
    "        elif self.agent_params['reward_type'] == 'mutual_info':\n",
    "            self.reward_func = self.calculate_mutual_infos\n",
    "        \n",
    "        self.data_params = params['data']\n",
    "        self.DataManager = DataManager(self.data_params, self.num_bands)\n",
    "        self.band_selection_num = self.data_params['band_selection_num']\n",
    "\n",
    "        self.critic_params = params['critic']\n",
    "        self.critic = QCritic(self.critic_params, self.num_bands)\n",
    "        \n",
    "        \n",
    "        self.policy_params = params['policy']\n",
    "        self.policy = ArgMaxPolicy(self.policy_params, self.critic)\n",
    "        \n",
    "        self.replay_buffer = ReplayBuffer()\n",
    "        \n",
    "        self.cache = {}\n",
    "        \n",
    "        self.logging_df = pd.DataFrame()\n",
    "        \n",
    "    \n",
    "    def generateTrajectories(self):\n",
    "        \n",
    "        #we expect paths to be a list of trajectories\n",
    "        #a trajectory is a list of Path objects\n",
    "        paths = []\n",
    "        for i in range(self.trajectory_sample_size):\n",
    "            \n",
    "            path = self.sampleTrajectory()\n",
    "#             print(f'Iter {i}')\n",
    "#             print([p['re'] for p in path])\n",
    "            \n",
    "            paths.append(path)\n",
    "    \n",
    "        return paths\n",
    "    \n",
    "    def sampleTrajectory(self, iter_num = 1):\n",
    "            \n",
    "        #select 30 actions\n",
    "        state = np.zeros(self.num_bands)\n",
    "        state_next = state.copy()\n",
    "        \n",
    "        #paths will be a list of dictionaries\n",
    "        path = []\n",
    "        for i in range(self.band_selection_num):\n",
    "            \n",
    "            action, action_type = self.policy.get_action(state)\n",
    "            state_next[action] += 1\n",
    "\n",
    "            reward, correlation_current_state, correlation_next_state = self.calculate_reward(state, state_next)\n",
    "\n",
    "            terminal = 1 if i == self.band_selection_num - 1 else 0\n",
    "            path.append(self.Path(state.copy(), action, state_next.copy(), reward, terminal))\n",
    "            \n",
    "            state = state_next.copy()\n",
    "        \n",
    "            if iter_num % 100 == 0:\n",
    "                print(\"Iter : \", iter_num)\n",
    "                q_values = self.critic.get_action(state)\n",
    "                \n",
    "                sampled_paths = self.replay_buffer.sample_buffer_random(1)\n",
    "                \n",
    "                flat_sampled_path = [path for trajectory in sampled_paths for path in trajectory]\n",
    "                obs = np.array([path['ob'] for path in flat_sampled_path])\n",
    "                acs = np.array([path['ac'] for path in flat_sampled_path])\n",
    "                obs_next = np.array([path['ob_next'] for path in flat_sampled_path])\n",
    "                res = np.array([path['re'] for path in flat_sampled_path])\n",
    "                terminals = np.array([path['terminal'] for path in flat_sampled_path])\n",
    "                \n",
    "                loss_value = self.critic.update(obs, acs, obs_next, res, terminals)\n",
    "                \n",
    "                row = {\n",
    "                    \"iter_num\": iter_num,\n",
    "                    \"Selected Band\": i,\n",
    "                    \"Action Type\": action_type,\n",
    "                    \"Mean\": torch.mean(q_values).detach().numpy(),\n",
    "                    \"Min\": torch.min(q_values).detach().numpy(),\n",
    "                    \"Max\": torch.max(q_values).detach().numpy(),\n",
    "                    \"Correlation Current State\" : correlation_current_state,\n",
    "                    \"Correlation Next State\" : correlation_next_state,\n",
    "                    \"Reward\" : reward,\n",
    "                    \"Loss\" : loss_value\n",
    "                }\n",
    "                \n",
    "                self.logging_df = self.logging_df.append(row, ignore_index=True)\n",
    "                \n",
    "                \n",
    "#                 print(self.logging_df)\n",
    "        \n",
    "        #path returns state, action, state_next, reward, terminal\n",
    "        return path\n",
    "                   \n",
    "        \n",
    "    def runAgent(self):\n",
    "        \n",
    "        for iter_num in range(self.n_iter):\n",
    "            \n",
    "            print('Iteration ', iter_num, ':')\n",
    "            \n",
    "            paths = self.generateTrajectories()\n",
    "            self.replay_buffer.add_trajectories(paths)\n",
    "            \n",
    "            for _ in range(self.num_critic_updates):\n",
    "                sampled_paths = self.replay_buffer.sample_buffer_random(self.agent_params['batch_size'])\n",
    "                \n",
    "                flat_sampled_path = [path for trajectory in sampled_paths for path in trajectory]\n",
    "                obs = np.array([path['ob'] for path in flat_sampled_path])\n",
    "                acs = np.array([path['ac'] for path in flat_sampled_path])\n",
    "                obs_next = np.array([path['ob_next'] for path in flat_sampled_path])\n",
    "                res = np.array([path['re'] for path in flat_sampled_path])\n",
    "                terminals = np.array([path['terminal'] for path in flat_sampled_path])\n",
    "                \n",
    "                critic_loss = self.critic.update(obs, acs, obs_next, res, terminals)\n",
    "                \n",
    "            self.critic.update_target_network()\n",
    "            \n",
    "            #sample a single trajectory\n",
    "            print('------------------------------------EVAL Results------------------------------')\n",
    "            eval_path = self.sampleTrajectory(iter_num)\n",
    "#             print(self.cache)\n",
    "            #print(eval_path[-1])\n",
    "            #print('Selected_Bands: ', np.argwhere(eval_path[-1]['ob_next']>0))\n",
    "            print('Num_Selected_Bands: ', np.argwhere(eval_path[-1]['ob_next']>0).shape[0])\n",
    "            print('Eval_Return: ', np.sum(eval_path[-1]['re']))\n",
    "            print('Critic_Loss: ', critic_loss)\n",
    "                \n",
    "\n",
    "    def calculate_reward(self, state, state_next):\n",
    "        #for future, save down the previous state so that we can avoid a calc\n",
    "        \n",
    "#         print(list(np.argwhere(np.array(state) != 0)), list(np.argwhere(np.array(state_next) != 0)))\n",
    "        if list(np.argwhere(np.array(state) != 0)) == list(np.argwhere(np.array(state_next) != 0)):\n",
    "#             print(\"same action selected\")\n",
    "            return -1, \"Indef\", \"Indef\"\n",
    "        else:\n",
    "        \n",
    "            a = self.reward_func(state)\n",
    "            b = self.reward_func(state_next)\n",
    "            return a-b, a, b\n",
    "    \n",
    "    \n",
    "    def calculate_correlations(self, state):\n",
    "        \n",
    "#         if repr(state) in self.cache:\n",
    "#             return self.cache[repr(state)]\n",
    "        \n",
    "        #deal with the first state\n",
    "        ##### THIS LOGIC SEEMS WRONG - REGARDLESS OF THE FIRST PICK, YOU HAVE A REWARD OF 0#####\n",
    "        if np.sum(state) <= 1:\n",
    "            return 0\n",
    "        \n",
    "        selected_bands = []\n",
    "        non_zero_bands = np.argwhere(np.array(state) != 0)\n",
    "        for band in non_zero_bands:\n",
    "#             print(band[0])\n",
    "            selected_bands.extend([band[0]]*int(state[band[0]]))\n",
    "        #print(selected_bands)\n",
    "        #selected_bands = np.squeeze(np.argwhere(np.array(state)==1))\n",
    "        corr_sum = 0\n",
    "        for idx_i, i in enumerate(selected_bands):\n",
    "            for idx_j, j in enumerate(selected_bands):\n",
    "                if idx_i != idx_j:\n",
    "                    \n",
    "                    if repr((i,j)) in self.cache:\n",
    "                        result = self.cache[repr((i,j))]\n",
    "                    else:\n",
    "                        result = abs(pearsonr(self.DataManager.rl_data[:, i], self.DataManager.rl_data[:, j])[0])\n",
    "                        self.cache[repr((i,j))] = result\n",
    "                    \n",
    "                    corr_sum += result\n",
    "                    \n",
    "#                     corr_sum += abs(pearsonr(self.DataManager.rl_data[:, i], self.DataManager.rl_data[:, j])[0])\n",
    "        \n",
    "#         self.cache[repr(state)] = corr_sum/(len(selected_bands)**2)\n",
    "        \n",
    "#         return self.cache[repr(state)]\n",
    "        return corr_sum/(len(selected_bands)**2)\n",
    "\n",
    "    def calculate_mutual_infos(self, state):\n",
    "    \n",
    "        selected_bands = []\n",
    "        non_zero_bands = np.argwhere(np.array(state) != 0)\n",
    "        for band in non_zero_bands:\n",
    "            selected_bands.extend([band[0]]*int(state[band[0]]))\n",
    "    \n",
    "        normalized_mutual_info_score_sum = 0\n",
    "        for i in selected_bands:\n",
    "            for j in selected_bands:\n",
    "\n",
    "                normalized_mutual_info_score_sum += normalized_mutual_info_score(self.DataManager.rl_data[:, i],\n",
    "                                                                                 self.DataManager.rl_data[:, j])\n",
    "\n",
    "        return normalized_mutual_info_score_sum/(len(selected_bands)**2)\n",
    "\n",
    "            \n",
    "    def Path(self, ob, ac, ob_next, re, terminal):\n",
    "        return {'ob':ob,\n",
    "                'ac':ac,\n",
    "                'ob_next':ob_next,\n",
    "                're':re,\n",
    "                'terminal':terminal\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb12fe7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArgMaxPolicy():\n",
    "    \n",
    "    def __init__(self, params, critic):\n",
    "        self.epsilon = params['epsilon']\n",
    "        self.epsilon_decay = params['epsilon_decay']\n",
    "        self.critic = critic\n",
    "        \n",
    "    def get_action(self, obs):\n",
    "        \n",
    "        q_value_estimates = self.critic.get_action(obs)\n",
    "        #print(obs)\n",
    "#         print('Predicted Q-Values:', q_value_estimates)\n",
    "        \n",
    "        rand = np.random.rand()\n",
    "        if rand < self.epsilon:\n",
    "            #select a random action\n",
    "#             print('Selected Random')\n",
    "            unselected_bands = np.squeeze(np.argwhere(obs == 0))\n",
    "            selected_idx = np.random.choice(unselected_bands)\n",
    "            action_type = \"Random Action\"\n",
    "\n",
    "        else:\n",
    "#             print('Selected Max')\n",
    "            selected_idx = torch.argmax(q_value_estimates).item()\n",
    "            action_type = \"Max Action\"\n",
    "\n",
    "            \n",
    "        self.decay_epsilon()\n",
    "        return selected_idx, action_type\n",
    "                \n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon *= self.epsilon_decay "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bcd7fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QCritic():\n",
    "    \n",
    "    def __init__(self, params, num_bands):\n",
    "        \n",
    "        self.num_bands = num_bands\n",
    "\n",
    "        \n",
    "        self.critic = self.create_network()\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(),lr=0.005)\n",
    "\n",
    "        self.critic_target = self.create_network()\n",
    "        \n",
    "        self.gamma = params['gamma']\n",
    "        \n",
    "        self.loss = nn.SmoothL1Loss()\n",
    "    \n",
    "    def create_network(self):\n",
    "        \n",
    "        q_net  = nn.Sequential(\n",
    "        nn.Linear(self.num_bands, self.num_bands*2),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(self.num_bands*2, self.num_bands*2),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(self.num_bands*2, self.num_bands)\n",
    "        )\n",
    "        \n",
    "        return q_net\n",
    "    \n",
    "        \n",
    "    def forward(self, obs):\n",
    "        # will take in one hot encoded states and output a list of qu values\n",
    "        \n",
    "        q_values = self.critic(obs)\n",
    "        \n",
    "        return q_values\n",
    "    \n",
    "    def get_action(self, obs):\n",
    "        \n",
    "        if isinstance(obs, np.ndarray):\n",
    "            obs = from_numpy(obs)\n",
    "            \n",
    "        return self.critic(obs)\n",
    "    \n",
    "    def update(self, obs, ac_n, next_obs, reward_n, terminals):\n",
    "        \n",
    "        obs = self.check_tensor(obs) #comes in as shape \n",
    "        ac_n = self.check_tensor(ac_n)\n",
    "        next_obs = self.check_tensor(next_obs)\n",
    "        reward_n = self.check_tensor(reward_n)\n",
    "        terminals = self.check_tensor(terminals)\n",
    "        \n",
    "        full_q_values = self.critic(obs)\n",
    "        q_actions = full_q_values.argmax(dim=1)\n",
    "        q_values = torch.gather(full_q_values, 1, q_actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        \n",
    "        #print('Obs ', obs.shape)\n",
    "        #print('Full Q ', full_q_values.shape)\n",
    "        #print('Q Actions ', q_actions.shape)\n",
    "        #print('Q Val ', q_values.shape)\n",
    "        \n",
    "        full_q_next_target = self.critic_target(next_obs)\n",
    "        q_actions_next = self.critic(next_obs).argmax(dim=1)\n",
    "        #q_values_next = full_q_next.max(dim=1)\n",
    "        #print('q_values_next', q_values_next)\n",
    "        q_values_next = torch.gather(full_q_next_target, 1, q_actions_next.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        #print('reward', type(reward_n.shape))\n",
    "        #print('q_values_next', type(q_values_next))\n",
    "        #print('terminals', type(terminals))\n",
    "        #print('gamma', type(self.gamma))\n",
    "        target = reward_n + self.gamma*q_values_next*(1-terminals)\n",
    "        target = target.detach()\n",
    "        \n",
    "        #print(f'Target Dim: {target.shape}')\n",
    "        #print(f'Q_Values Dim: {q_values.shape}')\n",
    "        loss = self.loss(q_values, target)\n",
    "        \n",
    "        self.critic_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def check_tensor(self, ar):\n",
    "        \n",
    "        if isinstance(ar, np.ndarray):\n",
    "            ar = from_numpy(ar)\n",
    "            \n",
    "        return ar\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        for target_param, param in zip(\n",
    "            self.critic_target.parameters(), self.critic.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "770adc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#utility functions\n",
    "## taken from Prof. Sergey Levine's CS285 HW\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "def from_numpy(*args, **kwargs):\n",
    "    return torch.from_numpy(*args, **kwargs).float().to(device)\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return tensor.to('cpu').detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6dbabbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'agent':{\n",
    "            'n_iter':2000,\n",
    "            'trajectory_sample_size': 10,\n",
    "            'batch_size':5,\n",
    "            'num_critic_updates':5,\n",
    "            'num_bands':200,\n",
    "            'reward_type':'mutual_info'\n",
    "            },\n",
    "          'data':{\n",
    "            'band_selection_num':30,\n",
    "            'dataset_type':'SalienObjects',\n",
    "            'data_file_path':r'../data/indian_pines/hyperspectral_imagery/indian_pines_corrected.npy',\n",
    "            },\n",
    "          'critic':{\n",
    "            'gamma':0.99\n",
    "            },\n",
    "          'policy':{\n",
    "            'epsilon':1,\n",
    "            'epsilon_decay':0.999\n",
    "            }\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e094aa21",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Agent' object has no attribute 'agent_param'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[43mAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m agent\u001b[38;5;241m.\u001b[39mrunAgent()\n",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36mAgent.__init__\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreward_type\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcorrelation\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward_func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalculate_correlations\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent_param\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreward_type\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmutual_info\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward_func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalculate_mutual_infos\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_params \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Agent' object has no attribute 'agent_param'"
     ]
    }
   ],
   "source": [
    "agent = Agent(params)\n",
    "agent.runAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d738c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.logging_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4510dd7e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(6):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    plot_df = agent.logging_df[agent.logging_df[\"Selected Band\"] == i]\n",
    "    ax1.plot(plot_df[\"iter_num\"], plot_df[\"Mean\"], color='red', label=\"Mean\")\n",
    "    ax1.plot(plot_df[\"iter_num\"], plot_df[\"Max\"], color='blue', label=\"Max\")\n",
    "    ax1.plot(plot_df[\"iter_num\"], plot_df[\"Min\"], color='green', label=\"Min\")\n",
    "    ax1.axhline(plot_df['Reward'].mean(), color='red')\n",
    "    ax1.set_title(f'Band Selection {i}')\n",
    "    ax1.legend()\n",
    "    ax2.plot(plot_df[\"iter_num\"], plot_df[\"Loss\"], color='red')\n",
    "    ax2.set_title('Loss Function')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9ea8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.logging_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5266626c",
   "metadata": {},
   "outputs": [],
   "source": [
    "band_1_logging_df = agent.logging_df[agent.logging_df[\"Selected Band\"] == 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cf17e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "band_1_logging_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0364bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(num = 3, figsize=(8, 5))\n",
    "plt.plot(band_1_logging_df[\"iter_num\"], band_1_logging_df[\"Mean\"], color='red', label=\"Mean\")\n",
    "plt.plot(band_1_logging_df[\"iter_num\"], band_1_logging_df[\"Max\"], color='blue', label=\"Max\")\n",
    "plt.plot(band_1_logging_df[\"iter_num\"], band_1_logging_df[\"Min\"], color='green', label=\"Min\")\n",
    "\n",
    "plt.axhline(band_1_logging_df['Reward'].mean( ))\n",
    "\n",
    "plt.xlabel(\"iter num\")\n",
    "plt.ylabel(\"Q values\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b01749",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(num = 3, figsize=(8, 5))\n",
    "plt.plot(band_1_logging_df[\"iter_num\"], band_1_logging_df[\"Loss\"], color='red')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8c5ec9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab703265",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/romitbarua/Documents/Berkeley/Fall 2022/CS285-Deep Reinforcement Learning/HyperSpectralRL/data/data_indian_pines_drl.mat'\n",
    "data = scipy.io.loadmat(path)['x']\n",
    "band_1 = data[:, 173]\n",
    "band_2 = data[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d444156",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = np.sum((band_1 - np.mean(band_1))*(band_2 - np.mean(band_2)))\n",
    "den = np.sqrt(np.sum((band_1 - np.mean(band_1))**2)*np.sum((band_2 - np.mean(band_2))**2))\n",
    "num/den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d935b982",
   "metadata": {},
   "outputs": [],
   "source": [
    "band_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb388942",
   "metadata": {},
   "outputs": [],
   "source": [
    "band_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27892760",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = []\n",
    "band_1 = data[:, 40]\n",
    "for i in range(2, 200):\n",
    "\n",
    "    band_2 = data[:, i]\n",
    "    num = np.sum((band_1 - np.mean(band_1))*(band_2 - np.mean(band_2)))\n",
    "    den = np.sqrt(np.sum((band_1 - np.mean(band_1))**2)*np.sum((band_2 - np.mean(band_2))**2))\n",
    "    corr.append(num/den)\n",
    "    \n",
    "    \n",
    "sns.histplot(corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2c9baa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214bdf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'/Users/pratikaher/FALL22/HyperSpectralRL/ForPratik/data_indian_pines_drl.mat'\n",
    "num_iter = 1000\n",
    "num_bands = 20\n",
    "bands_to_select = 5\n",
    "data = scipy.io.loadmat(path)['x'][:, :num_bands]\n",
    "\n",
    "all_band_selection = []\n",
    "all_corr = []\n",
    "all_reward = []\n",
    "\n",
    "for num_iter in range(num_iter):\n",
    "    selected_bands = list(np.random.choice(np.arange(0, num_bands, 1), 1))\n",
    "    corr = [0]\n",
    "    rewards = [0]\n",
    "    for i in range(bands_to_select):\n",
    "\n",
    "        selected_bands.extend(list(np.random.choice(np.arange(0, num_bands, 1), 1)))\n",
    "        corr_sum = 0\n",
    "        for idx_i, i in enumerate(selected_bands):\n",
    "            for idx_j, j in enumerate(selected_bands):\n",
    "                if idx_i != idx_j:\n",
    "                    corr_sum += abs(pearsonr(data[:, i], data[:, j])[0])\n",
    "        corr.append(corr_sum/(len(selected_bands)**2))\n",
    "        rewards.append(corr[-2] - corr[-1])\n",
    "    \n",
    "    if num_iter % 100 == 0:\n",
    "        print(num_iter)\n",
    "        \n",
    "    all_band_selection.append(selected_bands)\n",
    "    all_corr.append(corr)\n",
    "    all_reward.append(rewards)\n",
    "    \n",
    "all_band_selection = np.array(all_band_selection)\n",
    "all_corr = np.array(all_corr)\n",
    "all_reward = np.array(all_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4eeb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iter = 1000\n",
    "cum_reward = np.zeros((num_iter, bands_to_select))\n",
    "for i in range(bands_to_select):\n",
    "    cum_reward[:, i] = np.sum(all_reward[:, i+1:], axis=1)\n",
    "    \n",
    "# reasonbleness check to make sure this works\n",
    "#print(all_reward[0, :])\n",
    "#print(sum(all_reward[0, :]))\n",
    "#print(cum_reward[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ea32ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(bands_to_select):\n",
    "    sns.histplot(cum_reward[:,i])\n",
    "    plt.title(f'Distribution of Q-Values at band selection {i+1}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177c51fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "301faebbd5cea7fd4466786a19f1bea9d8baf657aaca95ef39840c46b8697603"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
